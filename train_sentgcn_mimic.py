from data import Biview_MultiSent, sent_collate_fn
from sentgcn import SentGCN
from evaluate import evaluate
from my_build_vocab import Vocabulary
import os
import math
import argparse
import logging
import pickle
import torch
import torch.nn as nn
from torch.nn.utils import clip_grad_value_
from torch.utils.data import DataLoader
from torch.utils.tensorboard.writer import SummaryWriter


def get_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--name', type=str, required=True)
    parser.add_argument('--model-dir', type=str, default='./models')
    parser.add_argument('--output-dir', type=str, default='./output')
    parser.add_argument('--pretrained', type=str, default='./models/mlclassifier_ones3_t012v3t4_lr1e-6_e95.pth')
    parser.add_argument('--checkpoint', type=str, default='')
    parser.add_argument('--dataset-dir', type=str, default='./data')
    parser.add_argument('--train-folds', type=str, default='012')
    parser.add_argument('--val-folds', type=str, default='3')
    parser.add_argument('--test-folds', type=str, default='4')
    parser.add_argument('--report-path', type=str, default='./data/reports.json')
    parser.add_argument('--vocab-path', type=str, default='./data/vocab.pkl')
    parser.add_argument('--label-path', type=str, default='./data/label_dict.json')
    parser.add_argument('--log-dir', type=str, default='./data/logs')
    parser.add_argument('--log-freq', type=int, default=1)
    parser.add_argument('--num-epochs', type=int, default=100)
    parser.add_argument('--seed', type=int, default=123)
    parser.add_argument('--encoder-lr', type=float, default=1e-6)
    parser.add_argument('--decoder-lr', type=float, default=1e-4)
    parser.add_argument('--batch-size', type=int, default=8)
    parser.add_argument('--gpus', type=str, default='0')
    parser.add_argument('--num_classes', type=int, default=20)
    parser.add_argument('--clip-value', type=float, default=5.0)

    args = parser.parse_args()

    return args


if __name__ == '__main__':
    args = get_args()

    os.makedirs(args.model_dir, exist_ok=True)
    os.makedirs(args.output_dir, exist_ok=True)
    os.makedirs(args.log_dir, exist_ok=True)

    logging.basicConfig(filename=os.path.join(args.log_dir, args.name + '.log'), level=logging.INFO)
    print('------------------------Model and Training Details--------------------------')
    print(args)
    for k, v in vars(args).items():
        logging.info('{}: {}'.format(k, v))

    writer = SummaryWriter(log_dir=os.path.join('./runs', args.name))

    gpus = [int(_) for _ in list(args.gpus)]
    device = torch.device('cuda:{}'.format(gpus[0]) if torch.cuda.is_available() else 'cpu')
    torch.manual_seed(args.seed)

    with open(args.vocab_path, 'rb') as f:
        vocab = pickle.load(f)

    train_set = Biview_MultiSent('train', args.dataset_dir, args.train_folds, args.report_path, args.vocab_path, args.label_path)
    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=8, collate_fn=sent_collate_fn)
    val_set = Biview_MultiSent('val', args.dataset_dir, args.val_folds, args.report_path, args.vocab_path, args.label_path)
    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=1, collate_fn=sent_collate_fn)
    test_set = Biview_MultiSent('test', args.dataset_dir, args.test_folds, args.report_path, args.vocab_path, args.label_path)
    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1, collate_fn=sent_collate_fn)

    fw_adj = torch.tensor([
[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] ,
[0.0, 0.0, 0.4561275751719785, 0.1312437281509043, 0.24146207792929056, 0.5508571762992422, 0.1562972606280271, 0.0, 0.33109282796069434, 0.0, 0.7450978096449961, 0.0, 0.09811571952749415, 0.3566126916516296, 0.4550427125255811, 0.0, 0.0, 0.0, 0.0, 0.3675326731479569, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0742334537640819, 0.0, 0.0, 0.04911539003545221] ,
[0.0, 0.4561275751719785, 0.0, 0.2239353858402398, 0.7210226952471281, 0.1813069680770458, 0.0788364926609991, 0.0, 0.15580397237632496, 0.0, 0.5213886531771248, 0.29374910091081574, 0.07338686671308393, 0.0, 0.47453376787334545, 0.0, 0.0, 0.0, 0.06713445127088644, 0.0, 0.2390801083216384, 0.0, 0.0, 0.5368211205907667, 0.0, 0.12777400002905853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.1312437281509043, 0.2239353858402398, 0.0, 0.6583894493157434, 0.4838989820766221, 0.0, 0.5838389742905232, 0.1802535145956454, 0.0, 0.7991245475975839, 0.11752099418944446, 1.06322936147921, 0.14944553664268462, 0.555751891083365, 0.0, 0.0, 0.0, 0.0, 0.0, 1.194781559134564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06242749638070836, 0.0, 0.0, 0.0] ,
[0.0, 0.24146207792929056, 0.7210226952471281, 0.6583894493157434, 0.0, 0.33739430398543896, 0.0, 0.0, 0.2753154828514788, 0.0, 0.7937543770840491, 0.0, 0.44113179799973046, 0.0, 0.3813008244727325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2757852680983795, 0.0, 0.0, 0.10440811928574718, 0.32527161563918205, 0.30268868564421475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4051634698397344, 0.0, 0.0, 0.0, 0.0, 0.23170964380526302] ,
[0.0, 0.5508571762992422, 0.1813069680770458, 0.4838989820766221, 0.33739430398543896, 0.0, 0.5084212742711277, 0.6675018415064685, 0.4573178232633596, 0.0, 0.47523016454835715, 0.655060126377754, 0.04554159429343136, 0.690421666641882, 0.558453872850085, 0.0, 0.0, 0.0, 0.0, 0.20852065737883554, 0.6848377491773682, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2185149665526724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04939763863424854] ,
[0.0, 0.1562972606280271, 0.0788364926609991, 0.0, 0.0, 0.5084212742711277, 0.0, 0.0, 0.0, 0.0, 0.005426229300043639, 1.3057145824116705, 0.0, 0.8349693844237195, 0.008921790018546403, 0.0, 0.0, 0.11331204695360785, 0.09144882429915285, 0.4600716943289044, 0.4835214391246888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02380115748556941, 2.1599410696116648, 0.5656942084851855, 0.0, 0.0, 0.0, 0.5385812163077162, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.5838389742905232, 0.0, 0.6675018415064685, 0.0, 0.0, 0.731672905950786, 0.0, 0.19873089907410066, 1.0717887940778525, 0.4314328875601624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.084615347661925, 0.0, 0.12484394581839403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13075929436986614, 0.0, 0.08945725659182202, 1.2587049974217708, 0.0, 0.0, 0.0, 0.7231133523008619, 0.24889659448837284, 0.0, 0.06274981602259916] ,
[0.0, 0.33109282796069434, 0.15580397237632496, 0.1802535145956454, 0.2753154828514788, 0.4573178232633596, 0.0, 0.731672905950786, 0.0, 0.0, 0.3658987804839403, 0.5684867664090824, 0.8076453675499128, 0.0, 0.24600870168688302, 0.0, 0.0, 0.0, 0.0, 0.0798570028567058, 0.6598408690505047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06942284468142029, 0.0, 0.0, 0.0, 0.6390156086235599, 0.0, 0.0, 0.2877393797550929] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37032811695643797, 0.5864507940803438, 0.09648724571559682, 0.08646096743214723, 0.0, 0.0, 0.3795124898565471, 0.5956873219192063, 0.0, 0.0, 0.029427346497570606, 0.3055219304384092, 0.44290494604282604, 0.18031374756716573, 0.0, 0.2817348748514817, 0.0, 0.3037353594240949, 0.0, 0.16902414094509366, 0.44897290997326605, 0.0] ,
[0.0, 0.7450978096449961, 0.5213886531771248, 0.7991245475975839, 0.7937543770840491, 0.47523016454835715, 0.005426229300043639, 0.19873089907410066, 0.3658987804839403, 0.0, 0.0, 0.15917680219264457, 0.23815070153292747, 0.32301546536533154, 0.4733720905109685, 0.0, 0.0, 0.0, 0.0, 0.3050316392916182, 0.0, 0.0, 0.0, 0.0, 0.007496236022830504, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1459218682394831, 0.0, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.0, 0.29374910091081574, 0.11752099418944446, 0.0, 0.655060126377754, 1.3057145824116705, 1.0717887940778525, 0.5684867664090824, 0.0, 0.15917680219264457, 0.0, 0.34332677414193835, 0.2635635043486729, 0.031005417323580087, 0.0, 0.0, 0.0, 0.09089149252550566, 0.02437348630097624, 0.0, 0.0, 0.0, 0.0, 0.03277105749999201, 0.0, 0.0, 0.0, 0.6391742933567508, 0.6037980683638654, 0.0, 0.0, 0.0033714685259266476, 0.23107114275084603, 0.0301477199380383, 0.0, 0.0] ,
[0.0, 0.09811571952749415, 0.07338686671308393, 1.06322936147921, 0.44113179799973046, 0.04554159429343136, 0.0, 0.4314328875601624, 0.8076453675499128, 0.0, 0.23815070153292747, 0.34332677414193835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.3533266696615023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5094212566296229, 0.0, 0.0, 0.0] ,
[0.0, 0.3566126916516296, 0.0, 0.14944553664268462, 0.0, 0.690421666641882, 0.8349693844237195, 0.0, 0.0, 0.0, 0.32301546536533154, 0.2635635043486729, 0.0, 0.0, 0.6931424489513275, 0.0, 0.0, 0.0, 0.0, 2.1074196113031913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15331793735154012, 0.23884807415186537, 0.0, 0.0, 0.0, 0.6251962447501228, 0.9033262867029686, 0.0, 0.0] ,
[0.0, 0.4550427125255811, 0.47453376787334545, 0.555751891083365, 0.3813008244727325, 0.558453872850085, 0.008921790018546403, 0.0, 0.24600870168688302, 0.0, 0.4733720905109685, 0.031005417323580087, 0.0, 0.6931424489513275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7161636297822183, 0.0, 0.0, 0.0, 0.0, 0.11876223494432343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7766379329419377] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37032811695643797, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.826263272280364, 0.572616403287778, 0.37608135779331225, 0.0, 0.0, 0.6664791879765191, 0.0, 0.0, 0.6695968919616625, 0.4488791110312937, 0.38945634739693075, 0.08411274303185438, 0.2917004403335538, 0.3800437779653869, 0.2380677791988365, 0.27679858458872236, 1.9549590442141598, 0.3722905991057948, 0.5307931536036093, 0.7368876097778745, 0.7061522884727808] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5864507940803438, 0.0, 0.0, 0.0, 0.0, 0.0, 2.826263272280364, 0.0, 0.35120934068491455, 0.2375133841107677, 0.0, 0.0, 0.5321065210844121, 0.19927245248782205, 0.0, 0.14957868637482777, 0.0, 0.25536621908603513, 0.0, 0.0, 0.0, 0.0, 0.03734498368480446, 2.516696010229012, 0.0, 0.5307889978371716, 0.0, 0.0960955758632178] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11331204695360785, 0.0, 0.0, 0.09648724571559682, 0.0, 0.0, 0.0, 0.0, 0.0, 0.572616403287778, 0.35120934068491455, 0.0, 0.0, 0.0, 0.0, 0.29348082706206574, 0.33245104866618147, 0.7834253853708174, 0.6637012531860743, 1.0870265661824885, 0.39863505669400867, 0.13312286942605217, 0.4347084018504075, 0.6789980152639449, 1.2829314721133003, 0.8828246864504936, 0.750212390764988, 0.3394040245033589, 0.17754003752713748, 0.3162719096872125, 0.0] ,
[0.0, 0.0, 0.06713445127088644, 0.0, 0.0, 0.0, 0.09144882429915285, 0.084615347661925, 0.0, 0.08646096743214723, 0.0, 0.09089149252550566, 0.0, 0.0, 0.0, 0.37608135779331225, 0.2375133841107677, 0.0, 0.0, 0.0, 1.6925691084401553, 0.3561732227230664, 0.027819402623596414, 0.7161674728252212, 0.5197157747849462, 0.8581083948136323, 0.42260822276000154, 0.3552253348597958, 0.5997658869837408, 0.720615333108806, 1.319266349117826, 0.5944386313930262, 0.7674724970376314, 0.693554702139206, 0.6387600967878838, 0.4217118279958016, 0.3030211175474031] ,
[0.0, 0.3675326731479569, 0.0, 0.0, 0.0, 0.20852065737883554, 0.4600716943289044, 0.0, 0.0798570028567058, 0.0, 0.3050316392916182, 0.02437348630097624, 0.0, 2.1074196113031913, 0.7161636297822183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29382396236601094, 0.0, 0.0, 0.17754549443031645, 0.0, 0.0, 0.32355364182848556, 0.159505799102812, 0.411251947423955, 0.52515309826666, 0.0, 0.596335294846914, 0.013806128482410229, 1.2531067143858092, 1.8230394564602632, 0.13362385380628916, 0.6001536689980314] ,
[0.0, 0.0, 0.2390801083216384, 1.194781559134564, 0.2757852680983795, 0.6848377491773682, 0.4835214391246888, 0.12484394581839403, 0.6598408690505047, 0.0, 0.0, 0.0, 1.3533266696615023, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6925691084401553, 0.29382396236601094, 0.0, 0.0, 0.0, 0.7138805959551021, 0.11297953392649797, 0.4844330162160724, 0.38596546185743924, 0.036239842558855984, 0.0, 0.5547948133103655, 1.0803235644904228, 0.0, 0.0, 0.7882036912676615, 0.0, 1.240751663702056, 1.0477606549141325] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3795124898565471, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6664791879765191, 0.5321065210844121, 0.29348082706206574, 0.3561732227230664, 0.0, 0.0, 0.0, 0.2534515169611425, 0.0, 0.11246945995471487, 0.07334273346935428, 0.21668590043388075, 0.21941094489706722, 0.9975080075129525, 0.368692533676505, 0.21812689267933633, 0.3287631475798967, 0.7093092033967436, 0.1917101851895807, 0.6056386787874534, 1.1599331154514032, 0.8043554214386265] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5956873219192063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19927245248782205, 0.33245104866618147, 0.027819402623596414, 0.0, 0.0, 0.2534515169611425, 0.0, 0.22725680593423186, 0.0, 0.0, 0.31555285272114897, 0.4267466727339617, 0.0, 0.1826978370690844, 0.0, 0.05379873696374713, 0.14234923917988185, 0.0, 0.08948062941912607, 0.20706340244730248, 0.0] ,
[0.0, 0.0, 0.5368211205907667, 0.0, 0.10440811928574718, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7834253853708174, 0.7161674728252212, 0.17754549443031645, 0.7138805959551021, 0.0, 0.22725680593423186, 0.0, 0.0, 0.4592383775815212, 0.16538956868582963, 0.7629051452564362, 0.0, 0.05346010889771364, 0.0, 0.27509540397847715, 0.21213865741151783, 0.0, 0.22482877173867202, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.32527161563918205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007496236022830504, 0.03277105749999201, 0.0, 0.0, 0.11876223494432343, 0.6695968919616625, 0.14957868637482777, 0.6637012531860743, 0.5197157747849462, 0.0, 0.11297953392649797, 0.11246945995471487, 0.0, 0.0, 0.0, 3.2134702267299677, 0.24101556699326157, 0.09145686480864357, 0.12919761705292898, 0.05039604762473017, 0.403553572864001, 1.3069750190832774, 0.7980425960864843, 0.47648926941152736, 1.6040951817884799, 0.0, 0.6259654177637075] ,
[0.0, 0.0, 0.12777400002905853, 0.0, 0.30268868564421475, 0.0, 0.0, 0.0, 0.0, 0.029427346497570606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4488791110312937, 0.0, 1.0870265661824885, 0.8581083948136323, 0.0, 0.4844330162160724, 0.07334273346935428, 0.0, 0.4592383775815212, 3.2134702267299677, 0.0, 0.2203493961718295, 0.018110233591376573, 0.10255522134088697, 0.26908677536189435, 0.7137634299128567, 1.6439628202548011, 0.925575399120091, 0.4163664995892516, 1.9116571827560045, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13075929436986614, 0.0, 0.3055219304384092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38945634739693075, 0.25536621908603513, 0.39863505669400867, 0.42260822276000154, 0.32355364182848556, 0.38596546185743924, 0.21668590043388075, 0.31555285272114897, 0.16538956868582963, 0.24101556699326157, 0.2203493961718295, 0.0, 0.4371114399604061, 0.5054285531888771, 0.44924515354001515, 0.3648403693789466, 0.28056669723711136, 0.4471627735959794, 0.7437313561632167, 0.4121877509333448, 0.2726999196691929, 0.18922614685864272] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.2185149665526724, 0.0, 0.0, 0.0, 0.44290494604282604, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08411274303185438, 0.0, 0.13312286942605217, 0.3552253348597958, 0.159505799102812, 0.036239842558855984, 0.21941094489706722, 0.4267466727339617, 0.7629051452564362, 0.09145686480864357, 0.018110233591376573, 0.4371114399604061, 0.0, 0.20271260668257918, 0.3668239365548465, 0.1184126119193528, 0.5934154784628667, 0.8376418968029923, 0.21619014199138478, 0.28461494476804783, 0.34161510088391334, 1.008623439095296] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02380115748556941, 0.08945725659182202, 0.0, 0.18031374756716573, 0.0, 0.6391742933567508, 0.0, 0.15331793735154012, 0.0, 0.2917004403335538, 0.0, 0.4347084018504075, 0.5997658869837408, 0.411251947423955, 0.0, 0.9975080075129525, 0.0, 0.0, 0.12919761705292898, 0.10255522134088697, 0.5054285531888771, 0.20271260668257918, 0.0, 0.8187047289562672, 0.1410302025894172, 0.3941373798499791, 0.300273095819943, 0.043970911938040466, 0.6307373897434683, 0.09322704591348681, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.1599410696116648, 1.2587049974217708, 0.06942284468142029, 0.0, 0.0, 0.6037980683638654, 0.0, 0.23884807415186537, 0.0, 0.3800437779653869, 0.0, 0.6789980152639449, 0.720615333108806, 0.52515309826666, 0.5547948133103655, 0.368692533676505, 0.1826978370690844, 0.05346010889771364, 0.05039604762473017, 0.26908677536189435, 0.44924515354001515, 0.3668239365548465, 0.8187047289562672, 0.0, 1.2341946413520681, 0.35781208803482784, 0.5890899178792679, 0.6340530114404032, 1.0585876788500728, 0.5803311766446675, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5656942084851855, 0.0, 0.0, 0.2817348748514817, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2380677791988365, 0.0, 1.2829314721133003, 1.319266349117826, 0.0, 1.0803235644904228, 0.21812689267933633, 0.0, 0.0, 0.403553572864001, 0.7137634299128567, 0.3648403693789466, 0.1184126119193528, 0.1410302025894172, 1.2341946413520681, 0.0, 1.5406418880016948, 0.7296881633928688, 0.0, 1.8831346562682474, 0.3277653544960015, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.4051634698397344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1459218682394831, 0.0, 0.0, 0.0, 0.0, 0.27679858458872236, 0.03734498368480446, 0.8828246864504936, 0.5944386313930262, 0.596335294846914, 0.0, 0.3287631475798967, 0.05379873696374713, 0.27509540397847715, 1.3069750190832774, 1.6439628202548011, 0.28056669723711136, 0.5934154784628667, 0.3941373798499791, 0.35781208803482784, 1.5406418880016948, 0.0, 2.051483967313616, 0.35982751520577233, 0.8118043924443167, 0.4208658024914072, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3037353594240949, 0.0, 0.0033714685259266476, 0.0, 0.0, 0.0, 1.9549590442141598, 2.516696010229012, 0.750212390764988, 0.7674724970376314, 0.013806128482410229, 0.0, 0.7093092033967436, 0.14234923917988185, 0.21213865741151783, 0.7980425960864843, 0.925575399120091, 0.4471627735959794, 0.8376418968029923, 0.300273095819943, 0.5890899178792679, 0.7296881633928688, 2.051483967313616, 0.0, 0.6607118414843173, 1.251934458983633, 0.6240485023131742, 0.12626765101490742] ,
[0.0, 0.0742334537640819, 0.0, 0.06242749638070836, 0.0, 0.0, 0.0, 0.7231133523008619, 0.6390156086235599, 0.0, 0.0, 0.23107114275084603, 0.5094212566296229, 0.6251962447501228, 0.0, 0.3722905991057948, 0.0, 0.3394040245033589, 0.693554702139206, 1.2531067143858092, 0.7882036912676615, 0.1917101851895807, 0.0, 0.0, 0.47648926941152736, 0.4163664995892516, 0.7437313561632167, 0.21619014199138478, 0.043970911938040466, 0.6340530114404032, 0.0, 0.35982751520577233, 0.6607118414843173, 0.0, 0.7989276546306372, 0.0, 0.7007917534875767] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5385812163077162, 0.24889659448837284, 0.0, 0.16902414094509366, 0.0, 0.0301477199380383, 0.0, 0.9033262867029686, 0.0, 0.5307931536036093, 0.5307889978371716, 0.17754003752713748, 0.6387600967878838, 1.8230394564602632, 0.0, 0.6056386787874534, 0.08948062941912607, 0.22482877173867202, 1.6040951817884799, 1.9116571827560045, 0.4121877509333448, 0.28461494476804783, 0.6307373897434683, 1.0585876788500728, 1.8831346562682474, 0.8118043924443167, 1.251934458983633, 0.7989276546306372, 0.0, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44897290997326605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7368876097778745, 0.0, 0.3162719096872125, 0.4217118279958016, 0.13362385380628916, 1.240751663702056, 1.1599331154514032, 0.20706340244730248, 0.0, 0.0, 0.0, 0.2726999196691929, 0.34161510088391334, 0.09322704591348681, 0.5803311766446675, 0.3277653544960015, 0.4208658024914072, 0.6240485023131742, 0.0, 0.0, 0.0, 0.0406426973721225] ,
[0.0, 0.04911539003545221, 0.0, 0.0, 0.23170964380526302, 0.04939763863424854, 0.0, 0.06274981602259916, 0.2877393797550929, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7766379329419377, 0.7061522884727808, 0.0960955758632178, 0.0, 0.3030211175474031, 0.6001536689980314, 1.0477606549141325, 0.8043554214386265, 0.0, 0.0, 0.6259654177637075, 0.0, 0.18922614685864272, 1.008623439095296, 0.0, 0.0, 0.0, 0.0, 0.12626765101490742, 0.7007917534875767, 0.0, 0.0406426973721225, 0.0] ,
    ], dtype=torch.float, device=device)
    bw_adj = fw_adj.t()

    model = SentGCN(args.num_classes, fw_adj, bw_adj, len(vocab)).to(device)
    for param in model.densenet121.parameters():
        param.requires_grad = False
    for param in model.cls_atten.parameters():
        param.requires_grad = False
    for param in model.gcn.parameters():
        param.requires_grad = False
    model.densenet121.eval()
    model.gcn.eval()
    decoder_params = \
        list(model.atten.parameters()) + \
        list(model.embed.parameters()) + \
        list(model.init_sent_h.parameters()) + \
        list(model.init_sent_c.parameters()) + \
        list(model.sent_lstm.parameters()) + \
        list(model.word_lstm.parameters()) + \
        list(model.fc.parameters())

    CELoss = nn.CrossEntropyLoss(reduction='none')

    optimizer = torch.optim.Adam(decoder_params, lr=args.decoder_lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [150], gamma=0.1)

    if args.pretrained:
        pretrained = torch.load(args.pretrained)
        pretrained_state_dict = pretrained['model_state_dict']
        state_dict = model.state_dict()
        state_dict.update({k: v for k, v in pretrained_state_dict.items() if k in state_dict and 'fc' not in k})
        model.load_state_dict(state_dict)

    start_epoch = 1
    if args.checkpoint:
        checkpoint = torch.load(args.checkpoint)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1

    if len(gpus) > 1:
        model = nn.DataParallel(model, device_ids=gpus)

    num_steps = math.ceil(len(train_set) / args.batch_size)

    val_gts = {}
    test_gts = {}

    for epoch in range(start_epoch, args.num_epochs + 1):
        model.dropout.train()
        epoch_loss = 0.0
        print('------------------------Training for Epoch {}---------------------------'.format(epoch))
        print('Learning rate {:.7f}'.format(optimizer.param_groups[0]['lr']))

        for i, (images1, images2, labels, captions, loss_masks, update_masks, caseids) in enumerate(train_loader):
            images1 = images1.to(device)
            images2 = images2.to(device)
            captions = captions.to(device)
            loss_masks = loss_masks.to(device).bool()
            update_masks = update_masks.to(device).bool()
            optimizer.zero_grad()
            logits = model(images1, images2, captions[:, :, :-1], update_masks)
            logits = logits.permute(0, 3, 1, 2).contiguous()
            captions = captions[:, :, 1:].contiguous()
            loss_masks = loss_masks[:, :, 1:].contiguous()
            loss = CELoss(logits, captions)
            loss = loss.masked_select(loss_masks).mean()
            loss.backward()
            epoch_loss += loss.item()
            clip_grad_value_(model.parameters(), args.clip_value)
            optimizer.step()

        epoch_loss /= num_steps
        print('Epoch {}/{}, Loss {:.4f}'.format(epoch, args.num_epochs, epoch_loss))
        writer.add_scalar('loss', epoch_loss, epoch)

        scheduler.step(epoch)

        if epoch % args.log_freq == 0:

            # save_fname = os.path.join(args.model_dir, '{}_e{}.pth'.format(args.name, epoch))
            # if len(gpus) > 1:
            #     state_dict = model.module.state_dict()
            # else:
            #     state_dict = model.state_dict()
            # torch.save({
            #     'epoch': epoch,
            #     'model_state_dict': state_dict,
            #     'optimizer_state_dict': optimizer.state_dict(),
            # }, save_fname)

            model.dropout.eval()
            val_res = {}
            for i, (images1, images2, labels, captions, loss_masks, update_masks, caseids) in enumerate(val_loader):
                images1 = images1.to(device)
                images2 = images2.to(device)
                preds = model(images1, images2, stop_id=vocab('.'))

                caseid = caseids[0]
                val_res[caseid] = ['']
                pred = preds[0].detach().cpu()
                for isent in range(pred.size(0)):
                    words = []
                    for wid in pred[isent].tolist():
                        w = vocab.idx2word[wid]
                        if w == '<start>' or w == '<pad>':
                            continue
                        if w == '<end>':
                            break
                        words.append(w)
                    val_res[caseid][0] += ' '.join(words)
                    val_res[caseid][0] += ' '
                if epoch == start_epoch:
                    val_gts[caseid] = ['']
                    cap = captions[0]
                    for isent in range(cap.size(0)):
                        words = []
                        for wid in cap[isent, 1:].tolist():
                            w = vocab.idx2word[wid]
                            if w == '<start>' or w == '<pad>':
                                continue
                            if w == '<end>':
                                break
                            words.append(w)
                        val_gts[caseid][0] += ' '.join(words)
                        val_gts[caseid][0] += ' '
            scores = evaluate(val_gts, val_res)
            writer.add_scalar('VAL BLEU 1', scores['Bleu_1'], epoch)
            writer.add_scalar('VAL BLEU 2', scores['Bleu_2'], epoch)
            writer.add_scalar('VAL BLEU 3', scores['Bleu_3'], epoch)
            writer.add_scalar('VAL BLEU 4', scores['Bleu_4'], epoch)
            writer.add_scalar('VAL ROUGE_L', scores['ROUGE_L'], epoch)
            writer.add_scalar('VAL CIDEr', scores['CIDEr'], epoch)
            # writer.add_scalar('VAL Meteor', scores['METEOR'], epoch)

            test_res = {}
            for i, (images1, images2, labels, captions, loss_masks, update_masks, caseids) in enumerate(test_loader):
                images1 = images1.to(device)
                images2 = images2.to(device)
                preds = model(images1, images2, stop_id=vocab('.'))

                caseid = caseids[0]
                test_res[caseid] = ['']
                pred = preds[0].detach().cpu()
                for isent in range(pred.size(0)):
                    words = []
                    for wid in pred[isent].tolist():
                        w = vocab.idx2word[wid]
                        if w == '<start>' or w == '<pad>':
                            continue
                        if w == '<end>':
                            break
                        words.append(w)
                    test_res[caseid][0] += ' '.join(words)
                    test_res[caseid][0] += ' '

                if epoch == start_epoch:
                    test_gts[caseid] = ['']
                    cap = captions[0]
                    for isent in range(cap.size(0)):
                        words = []
                        for wid in cap[isent, 1:].tolist():
                            w = vocab.idx2word[wid]
                            if w == '<start>' or w == '<pad>':
                                continue
                            if w == '<end>':
                                break
                            words.append(w)
                        test_gts[caseid][0] += ' '.join(words)
                        test_gts[caseid][0] += ' '

            scores = evaluate(test_gts, test_res)
            writer.add_scalar('TEST BLEU 1', scores['Bleu_1'], epoch)
            writer.add_scalar('TEST BLEU 2', scores['Bleu_2'], epoch)
            writer.add_scalar('TEST BLEU 3', scores['Bleu_3'], epoch)
            writer.add_scalar('TEST BLEU 4', scores['Bleu_4'], epoch)
            writer.add_scalar('TEST ROUGE_L', scores['ROUGE_L'], epoch)
            writer.add_scalar('TEST CIDEr', scores['CIDEr'], epoch)
            # writer.add_scalar('TEST Meteor', scores['METEOR'], epoch)

            with open(os.path.join(args.output_dir, '{}_test_e{}.csv'.format(args.name, epoch)), 'w') as f1:
                with open(os.path.join(args.output_dir, '{}_test_gts.csv'.format(args.name)), 'w') as f2:
                    for caseid in test_res.keys():
                        f1.write(test_res[caseid][0] + '\n')
                        f2.write(test_gts[caseid][0] + '\n')

    writer.close()

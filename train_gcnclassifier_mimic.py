from data import Biview_Classification
from mlclassifier import GCNClassifier
from my_build_vocab import Vocabulary
import os
import math
import argparse
import logging
import pickle
import time
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard.writer import SummaryWriter
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score


def get_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--name', type=str, required=True)
    parser.add_argument('--model-path', type=str, default='./models')
    parser.add_argument('--pretrained', type=str, default='./models/pretrained/model_ones_3epoch_densenet.tar')
    parser.add_argument('--checkpoint', type=str, default='')
    parser.add_argument('--dataset-dir', type=str, default='./data')
    parser.add_argument('--train-folds', type=str, default='012')
    parser.add_argument('--val-folds', type=str, default='3')
    parser.add_argument('--test-folds', type=str, default='4')
    parser.add_argument('--report-path', type=str, default='./data/mimic/reports.json')
    parser.add_argument('--vocab-path', type=str, default='./data/vocab.pkl')
    parser.add_argument('--label-path', type=str, default='./data/mimic/label.json')
    parser.add_argument('--log-path', type=str, default='./data/mimic/logs')
    parser.add_argument('--log-freq', type=int, default=3)
    parser.add_argument('--num-epochs', type=int, default=100)
    parser.add_argument('--seed', type=int, default=123)
    parser.add_argument('--lr', type=float, default=1e-6)
    parser.add_argument('--batch-size', type=int, default=8)
    parser.add_argument('--gpus', type=str, default='0')
    parser.add_argument('--clip-value', type=float, default=5.0)
    parser.add_argument('--num-classes', type=int, default=36)

    args = parser.parse_args()

    return args


if __name__ == '__main__':
    args = get_args()

    os.makedirs(args.model_path, exist_ok=True)
    os.makedirs(args.log_path, exist_ok=True)

    logging.basicConfig(filename=os.path.join(args.log_path, args.name + '.log'), level=logging.INFO)
    print('------------------------Model and Training Details--------------------------')
    print(args)
    for k, v in vars(args).items():
        logging.info('{}: {}'.format(k, v))

    writer = SummaryWriter(log_dir=os.path.join('/runs', args.name))

    device = torch.device('cuda:{}'.format(args.gpus[0]) if torch.cuda.is_available() else 'cpu')
    gpus = [int(_) for _ in list(args.gpus)]
    torch.manual_seed(args.seed)

    with open('./data/mimic/36class_keywords.txt') as f:
        keywords = f.read().splitlines()
    # keywords.append('other')

    train_set = Biview_Classification('train', args.dataset_dir, args.train_folds, args.label_path)
    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=8)
    val_set = Biview_Classification('val', args.dataset_dir, args.val_folds, args.label_path)
    val_loader = DataLoader(val_set, batch_size=1, shuffle=False, num_workers=1)
    test_set = Biview_Classification('test', args.dataset_dir, args.test_folds, args.label_path)
    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1)

    fw_adj = torch.tensor([
[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] ,
[0.0, 0.0, 0.5639399561643222, 0.2953811219891957, 0.26476311545757164, 0.7259223755566311, 0.25663739528181734, 0.004353389033039028, 0.5221580986263324, 0.0, 0.9674665062572337, 0.0, 0.2581313109321809, 0.4588824446015732, 0.6099782324171493, 0.0, 0.0, 0.1603022843762365, 0.0, 0.6643489858589626, 0.0, 0.0, 0.0, 0.0, 0.16261437581437846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23448954049240578, 0.0, 0.3710497664750876, 0.0, 0.0, 0.34593170274645807] ,
[0.0, 0.5639399561643222, 0.0, 0.0, 0.8807747584486454, 0.5977998783105353, 0.1511973755215004, 0.0, 0.27480154654173333, 0.0, 0.6246502107882371, 0.46541579614594336, 0.10194562210630617, 0.010763338359676857, 0.6512359406727317, 0.0, 0.0, 0.108128406325592, 0.5384781741061396, 0.14797758562841987, 0.7104238311568912, 0.0, 0.0, 1.0081648434260198, 0.26664968728769095, 0.5991177228643115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34995512512357657, 0.0, 0.16171528707927924, 0.0, 0.0, 0.10379843149356464] ,
[0.0, 0.2953811219891957, 0.0, 0.0, 0.49841927276960823, 0.47710185481575695, 0.0, 0.7403864779047334, 0.500221881646368, 0.0, 0.8051372806619213, 0.08924843663301739, 1.0890328844822006, 0.15098399972408164, 0.3556050210194132, 0.0, 0.0, 0.0, 0.0, 0.31447993436705024, 1.8531803765372339, 0.0, 0.0, 0.0, 0.13363550592753753, 0.0, 0.13117030356295828, 0.0, 0.0, 0.0, 0.0, 0.07453930293467162, 0.0, 0.7208263137833781, 0.0, 0.0, 0.3639698911012933] ,
[0.0, 0.26476311545757164, 0.8807747584486454, 0.49841927276960823, 0.0, 0.48710251786550035, 0.0, 0.0, 0.448668760519124, 0.0, 0.9270276645364679, 0.0, 0.436295871093856, 0.0, 0.3345206851772412, 0.0, 0.0, 0.0, 0.05326620019133447, 0.0, 0.5709840018462023, 0.0, 0.0, 0.39960685303356985, 0.6204703493870048, 0.5978874193920374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7003622035875571, 0.11091475970318618, 0.06846484023796495, 0.0, 0.0, 0.526908377553086] ,
[0.0, 0.7259223755566311, 0.5977998783105353, 0.47710185481575695, 0.48710251786550035, 0.0, 0.6126614043828456, 0.6399643087465201, 0.6281784408744471, 0.0, 0.5961693703611365, 1.0462211001686854, 0.18196015390603645, 0.7655847045113483, 0.9254262740110724, 0.0, 0.0, 0.039720085343859124, 0.39080713315715915, 0.7838214563110699, 1.2601385481096024, 0.045399169644693815, 0.0, 0.26425910553509735, 0.0, 0.0, 0.0, 0.7938157654849067, 0.08803972398265018, 0.047649478752174415, 0.0, 0.07131150959166142, 0.0, 0.22002246612310114, 0.0, 0.356517668976499, 0.6246984375664828] ,
[0.0, 0.25663739528181734, 0.1511973755215004, 0.0, 0.0, 0.6126614043828456, 0.0, 0.0, 0.0, 0.0, 0.036885050595966536, 1.082646642993092, 0.0, 1.0500715266129148, 0.21744920561642922, 0.0, 0.0, 0.21300481088546305, 0.19114158823100844, 0.55976445826076, 0.5832142030565444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12349392141742485, 2.2596338335435204, 0.665386972417041, 0.0, 0.0, 0.0, 0.6382739802395718, 0.016408084952866622, 0.0] ,
[0.0, 0.004353389033039028, 0.0, 0.7403864779047334, 0.0, 0.6399643087465201, 0.0, 0.0, 0.9008835804381274, 0.0, 0.265446530769869, 1.1020443678496534, 0.4656536486262203, 0.0, 0.0, 0.05434738426656475, 0.0, 0.036029225492419555, 0.1569182216484723, 0.0, 0.1971468198049414, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2030621683564134, 0.0, 0.16176013057836933, 1.331007871408318, 0.0, 0.0, 0.06965497002159711, 0.7954162262874093, 0.3211994684749203, 0.0033203546649397176, 0.13505269000914655] ,
[0.0, 0.5221580986263324, 0.27480154654173333, 0.500221881646368, 0.448668760519124, 0.6281784408744471, 0.0, 0.9008835804381274, 0.0, 0.0, 0.5371691760937949, 0.7760818079846526, 0.8804978973190237, 0.06499737372864202, 0.39660488904698693, 0.0, 0.0, 0.024296140349849768, 0.0, 0.19801238166575263, 0.7779962478595516, 0.0, 0.0, 0.024070790806389075, 0.02711828407940792, 0.0, 0.04211508683433339, 0.0, 0.028323745234112137, 0.18757822349046727, 0.0, 0.0, 0.0, 0.7571709874326069, 0.0, 0.0, 0.4058947585641401] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10842805170704962, 0.3245507288309557, 0.0, 0.0, 0.0, 0.0, 0.11761242460715876, 0.33378725666981773, 0.0, 0.0, 0.0, 0.04362186518902089, 0.18100488079343766, 0.0, 0.0, 0.019834809602093318, 0.0, 0.04183529417470648, 0.0, 0.0, 0.18707284472387745, 0.0] ,
[0.0, 0.9674665062572337, 0.6246502107882371, 0.8051372806619213, 0.9270276645364679, 0.5961693703611365, 0.036885050595966536, 0.265446530769869, 0.5371691760937949, 0.0, 0.0, 0.29080021772832865, 0.2693144077726637, 0.550453575514039, 0.5640000038161557, 0.0, 0.0, 0.0, 0.0, 0.6788038175302408, 0.13385808673855668, 0.0, 0.0, 0.04302628446402658, 0.3812684142614528, 0.24058565660791614, 0.0, 0.0, 0.16718731822516555, 0.0, 0.0, 0.5196940464781057, 0.032049831689907576, 0.18924145043967483, 0.14191182154565354, 0.0, 0.1935792561270836] ,
[0.0, 0.0, 0.46541579614594336, 0.08924843663301739, 0.0, 1.0462211001686854, 1.082646642993092, 1.1020443678496534, 0.7760818079846526, 0.0, 0.29080021772832865, 0.0, 0.5342108993710134, 0.33454668337822835, 0.03018854614903246, 0.0, 0.0, 0.0, 0.38244333667932345, 0.31592533045479376, 0.0, 0.0, 0.0, 0.0, 0.32432290165380945, 0.19068143488637815, 0.05478747500862793, 0.0, 0.9307261375105684, 0.8953499125176831, 0.17566290083775526, 0.27416162760616397, 0.29492331267974436, 0.5226229869046635, 0.3216995640918558, 0.0, 0.0] ,
[0.0, 0.2581313109321809, 0.10194562210630617, 1.0890328844822006, 0.436295871093856, 0.18196015390603645, 0.0, 0.4656536486262203, 0.8804978973190237, 0.0, 0.2693144077726637, 0.5342108993710134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.4526781599696872, 0.0, 0.0, 0.0849245537230536, 0.0, 0.0, 0.08888830885369649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6087727469378079, 0.0, 0.0, 0.0] ,
[0.0, 0.4588824446015732, 0.010763338359676857, 0.15098399972408164, 0.0, 0.7655847045113483, 1.0500715266129148, 0.0, 0.06499737372864202, 0.0, 0.550453575514039, 0.33454668337822835, 0.0, 0.0, 1.0000366946703592, 0.0, 0.0, 0.0, 0.0, 2.4452757280446877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4911740540930364, 0.5767041908933618, 0.0, 0.0, 0.0, 0.9630523614916189, 1.241182403444465, 0.0, 0.0] ,
[0.0, 0.6099782324171493, 0.6512359406727317, 0.3556050210194132, 0.3345206851772412, 0.9254262740110724, 0.21744920561642922, 0.0, 0.39660488904698693, 0.0, 0.5640000038161557, 0.03018854614903246, 0.0, 1.0000366946703592, 0.0, 0.22242921220349066, 0.2490964142503554, 0.0, 0.0, 1.2642671082725843, 0.0, 0.07022824029814745, 0.0, 0.1261791238151257, 0.6668657134346896, 0.49164511409254263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.35358303686509235, 0.14858366718873192, 0.0, 1.3247414114323037] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05434738426656475, 0.0, 0.10842805170704962, 0.0, 0.0, 0.0, 0.0, 0.22242921220349066, 0.0, 2.389754194952854, 0.13610732596026798, 0.0, 0.0, 0.0, 0.22997011064900927, 0.0, 0.0, 0.23308781463415273, 0.012370033703783923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.51844996688665, 0.0, 0.0942840762760994, 0.30037853245036483, 0.26964321114527107] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3245507288309557, 0.0, 0.0, 0.0, 0.0, 0.2490964142503554, 2.389754194952854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09559744375690267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0801869329015026, 0.0, 0.09427992050966208, 0.0, 0.0] ,
[0.0, 0.1603022843762365, 0.108128406325592, 0.0, 0.0, 0.039720085343859124, 0.21300481088546305, 0.036029225492419555, 0.024296140349849768, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13610732596026798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34691630804330764, 0.22719217585856444, 0.6505174888549788, 0.0, 0.0, 0.0, 0.24248893793643517, 0.8464223947857905, 0.4463156091229838, 0.31370331343747815, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.0, 0.5384781741061396, 0.0, 0.05326620019133447, 0.39080713315715915, 0.19114158823100844, 0.1569182216484723, 0.0, 0.0, 0.0, 0.38244333667932345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2560600311126453, 0.0, 0.0, 0.27965839549771154, 0.08320669745743656, 0.4215993174861227, 0.0, 0.0, 0.16325680965623096, 0.2841062557812964, 0.8827572717903163, 0.15792955406551648, 0.3309634197101218, 0.25704562481169607, 0.2022510194603742, 0.0, 0.0] ,
[0.0, 0.6643489858589626, 0.14797758562841987, 0.31447993436705024, 0.0, 0.7838214563110699, 0.55976445826076, 0.0, 0.19801238166575263, 0.0, 0.6788038175302408, 0.31592533045479376, 0.0, 2.4452757280446877, 1.2642671082725843, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08864402093915011, 0.0, 0.1598262175194041, 0.0, 0.8165976370582996, 1.3865303791327535, 0.0, 0.16364459167052153] ,
[0.0, 0.0, 0.7104238311568912, 1.8531803765372339, 0.5709840018462023, 1.2601385481096024, 0.5832142030565444, 0.1971468198049414, 0.7779962478595516, 0.0, 0.13385808673855668, 0.0, 1.4526781599696872, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2560600311126453, 0.0, 0.0, 0.0, 0.0, 0.2773715186275924, 0.0, 0.04792393888856274, 0.0, 0.0, 0.0, 0.1182857359828558, 0.6438144871629131, 0.0, 0.0, 0.3516946139401519, 0.0, 0.804242586374546, 0.6112515775866229] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.045399169644693815, 0.0, 0.0, 0.0, 0.11761242460715876, 0.0, 0.0, 0.0, 0.0, 0.07022824029814745, 0.22997011064900927, 0.09559744375690267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5609989301854428, 0.0, 0.0, 0.0, 0.2728001260692339, 0.0, 0.1691296014599437, 0.7234240381238934, 0.3678463441111166] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33378725666981773, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.0, 1.0081648434260198, 0.0, 0.39960685303356985, 0.26425910553509735, 0.0, 0.0, 0.024070790806389075, 0.0, 0.04302628446402658, 0.0, 0.0849245537230536, 0.0, 0.1261791238151257, 0.0, 0.0, 0.34691630804330764, 0.27965839549771154, 0.0, 0.2773715186275924, 0.0, 0.0, 0.0, 0.0, 0.02272930025401169, 0.0, 0.3263960679289264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.16261437581437846, 0.26664968728769095, 0.13363550592753753, 0.6204703493870048, 0.0, 0.0, 0.0, 0.02711828407940792, 0.0, 0.3812684142614528, 0.32432290165380945, 0.0, 0.0, 0.6668657134346896, 0.23308781463415273, 0.0, 0.22719217585856444, 0.08320669745743656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.776961149402458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.870465941755768, 0.36153351875897477, 0.03998019208401777, 1.1675861044609703, 0.0, 0.18945634043619783] ,
[0.0, 0.0, 0.5991177228643115, 0.0, 0.5978874193920374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24058565660791614, 0.19068143488637815, 0.0, 0.0, 0.49164511409254263, 0.012370033703783923, 0.0, 0.6505174888549788, 0.4215993174861227, 0.0, 0.04792393888856274, 0.0, 0.0, 0.02272930025401169, 2.776961149402458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2772543525853471, 1.2074537429272914, 0.4890663217925812, 0.0, 1.475148105428495, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.13117030356295828, 0.0, 0.0, 0.0, 0.2030621683564134, 0.04211508683433339, 0.04362186518902089, 0.0, 0.05478747500862793, 0.08888830885369649, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06891947586136758, 0.012736076212505451, 0.0, 0.0, 0.010653696268469971, 0.3072222788357068, 0.0, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.7938157654849067, 0.0, 0.0, 0.0, 0.18100488079343766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3263960679289264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15690640113535695, 0.4011328194754826, 0.0, 0.0, 0.0, 0.5721143617677863] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.08803972398265018, 0.12349392141742485, 0.16176013057836933, 0.028323745234112137, 0.0, 0.16718731822516555, 0.9307261375105684, 0.0, 0.4911740540930364, 0.0, 0.0, 0.0, 0.0, 0.16325680965623096, 0.0, 0.0, 0.5609989301854428, 0.0, 0.0, 0.0, 0.0, 0.06891947586136758, 0.0, 0.0, 0.3821956516287577, 0.0, 0.0, 0.0, 0.0, 0.19422831241595875, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.047649478752174415, 2.2596338335435204, 1.331007871408318, 0.18757822349046727, 0.0, 0.0, 0.8953499125176831, 0.0, 0.5767041908933618, 0.0, 0.0, 0.0, 0.24248893793643517, 0.2841062557812964, 0.08864402093915011, 0.1182857359828558, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012736076212505451, 0.0, 0.3821956516287577, 0.0, 0.7976855640245584, 0.0, 0.15258084055175827, 0.19754393411289362, 0.6220786015225631, 0.14382209931715767, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.665386972417041, 0.0, 0.0, 0.019834809602093318, 0.0, 0.17566290083775526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8464223947857905, 0.8827572717903163, 0.0, 0.6438144871629131, 0.0, 0.0, 0.0, 0.0, 0.2772543525853471, 0.0, 0.0, 0.0, 0.7976855640245584, 0.0, 1.104132810674185, 0.2931790860653591, 0.0, 1.4466255789407376, 0.0, 0.0] ,
[0.0, 0.23448954049240578, 0.34995512512357657, 0.07453930293467162, 0.7003622035875571, 0.07131150959166142, 0.0, 0.0, 0.0, 0.0, 0.5196940464781057, 0.27416162760616397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4463156091229838, 0.15792955406551648, 0.1598262175194041, 0.0, 0.0, 0.0, 0.0, 0.870465941755768, 1.2074537429272914, 0.0, 0.15690640113535695, 0.0, 0.0, 1.104132810674185, 0.0, 1.6149748899861065, 0.0, 0.37529531511680697, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.11091475970318618, 0.0, 0.0, 0.06965497002159711, 0.0, 0.04183529417470648, 0.032049831689907576, 0.29492331267974436, 0.0, 0.0, 0.0, 1.51844996688665, 2.0801869329015026, 0.31370331343747815, 0.3309634197101218, 0.0, 0.0, 0.2728001260692339, 0.0, 0.0, 0.36153351875897477, 0.4890663217925812, 0.010653696268469971, 0.4011328194754826, 0.0, 0.15258084055175827, 0.2931790860653591, 1.6149748899861065, 0.0, 0.22420276415680734, 0.8154253816561233, 0.1875394249856644, 0.0] ,
[0.0, 0.3710497664750876, 0.16171528707927924, 0.7208263137833781, 0.06846484023796495, 0.22002246612310114, 0.0, 0.7954162262874093, 0.7571709874326069, 0.0, 0.18924145043967483, 0.5226229869046635, 0.6087727469378079, 0.9630523614916189, 0.35358303686509235, 0.0, 0.0, 0.0, 0.25704562481169607, 0.8165976370582996, 0.3516946139401519, 0.0, 0.0, 0.0, 0.03998019208401777, 0.0, 0.3072222788357068, 0.0, 0.0, 0.19754393411289362, 0.0, 0.0, 0.22420276415680734, 0.0, 0.36241857730312754, 0.0, 0.26428267616006706] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6382739802395718, 0.3211994684749203, 0.0, 0.0, 0.14191182154565354, 0.3216995640918558, 0.0, 1.241182403444465, 0.14858366718873192, 0.0942840762760994, 0.09427992050966208, 0.0, 0.2022510194603742, 1.3865303791327535, 0.0, 0.1691296014599437, 0.0, 0.0, 1.1675861044609703, 1.475148105428495, 0.0, 0.0, 0.19422831241595875, 0.6220786015225631, 1.4466255789407376, 0.37529531511680697, 0.8154253816561233, 0.36241857730312754, 0.0, 0.0, 0.0] ,
[0.0, 0.0, 0.0, 0.0, 0.0, 0.356517668976499, 0.016408084952866622, 0.0033203546649397176, 0.0, 0.18707284472387745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.30037853245036483, 0.0, 0.0, 0.0, 0.0, 0.804242586374546, 0.7234240381238934, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14382209931715767, 0.0, 0.0, 0.1875394249856644, 0.0, 0.0, 0.0, 0.0] ,
[0.0, 0.34593170274645807, 0.10379843149356464, 0.3639698911012933, 0.526908377553086, 0.6246984375664828, 0.0, 0.13505269000914655, 0.4058947585641401, 0.0, 0.1935792561270836, 0.0, 0.0, 0.0, 1.3247414114323037, 0.26964321114527107, 0.0, 0.0, 0.0, 0.16364459167052153, 0.6112515775866229, 0.3678463441111166, 0.0, 0.0, 0.18945634043619783, 0.0, 0.0, 0.5721143617677863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26428267616006706, 0.0, 0.0, 0.0],
    ], dtype=torch.float, device=device)
    bw_adj = fw_adj.t()

    model = GCNClassifier(args.num_classes, fw_adj, bw_adj).to(device)
    if args.pretrained != '':
        checkpoint = torch.load(args.pretrained)
        pretrained_state_dict = checkpoint['state_dict']
        model_state_dict = model.state_dict()
        model_state_dict.update({k[7:]: v for k, v in pretrained_state_dict.items() if k[7:] in model_state_dict})
        model.load_state_dict(model_state_dict)

    BCELoss = nn.BCEWithLogitsLoss(pos_weight=train_set.get_class_weights()).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [150], gamma=0.1)

    start_epoch = 1
    if args.checkpoint != '':
        checkpoint = torch.load(args.checkpoint)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1

    if len(gpus) > 1:
        model = nn.DataParallel(model, device_ids=gpus)

    num_steps = math.ceil(len(train_set) / args.batch_size)

    for epoch in range(start_epoch, args.num_epochs + 1):
        model.train()
        epoch_loss = 0.0
        print('------------------------Training for Epoch {}---------------------------'.format(epoch))
        print('learning rate {:.7f}'.format(optimizer.param_groups[0]['lr']))
        count=0
        for i, (images1, images2, labels) in enumerate(train_loader):
            images1, images2, labels = images1.to(device), images2.to(device), labels.to(device)
            optimizer.zero_grad()
            logits = model(images1, images2)
            loss = BCELoss(logits, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            # # for test
            # count+=1
            # if count >5:
            #     break

        epoch_loss /= num_steps
        print('Epoch {}/{}, Loss {:.4f}'.format(epoch, args.num_epochs, epoch_loss))
        writer.add_scalar('train_loss', epoch_loss, epoch)

        scheduler.step(epoch)

        if epoch % args.log_freq == 0:

            save_fname = os.path.join(args.model_path, '{}_e{}.pth'.format(args.name, epoch))
            if len(gpus) > 1:
                state_dict = model.module.state_dict()
            else:
                state_dict = model.state_dict()
            torch.save({
                'epoch': epoch,
                'model_state_dict': state_dict,
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': epoch_loss
            }, save_fname)

            # evaluate
            count=0
            model.eval()
            y = torch.zeros((len(val_set), args.num_classes), dtype=torch.int)
            y_score = torch.zeros((len(val_set), args.num_classes), dtype=torch.float)
            with torch.no_grad():
                for i, (images1, images2, labels) in enumerate(val_loader):
                    images1, images2 = images1.to(device), images2.to(device)
                    scores = torch.sigmoid(model(images1, images2)).detach().cpu()
                    y[i] = labels[0]
                    y_score[i] = scores[0]
                    # # for test
                    # count+=1
                    # if count >10:
                    #     break
            y_hat = (y_score >= 0.5).type(torch.int)
            y = y.numpy()
            y_score = y_score.numpy()
            y_hat = y_hat.numpy()
            precision, recall, f, _ = precision_recall_fscore_support(y, y_hat)
            try:
              roc_auc = roc_auc_score(y, y_score, average=None)
              print('Epoch {}/{}, P {:.4f}, R {:.4f}, F {:.4f}, AUC {:.4f}'.format(
                  epoch, args.num_epochs, precision.mean(), recall.mean(), f.mean(), roc_auc.mean()))
              writer.add_scalar('precision', precision[:10].mean(), epoch)
              writer.add_scalar('recall', recall[:10].mean(), epoch)
              writer.add_scalar('f', f[:10].mean(), epoch)
              writer.add_scalar('auc', roc_auc.mean(), epoch)
            except ValueError:
              pass

            # test
            count=0
            y = torch.zeros((len(test_set), args.num_classes), dtype=torch.int)
            y_score = torch.zeros((len(test_set), args.num_classes), dtype=torch.float)
            with torch.no_grad():
                for i, (images1, images2, labels) in enumerate(test_loader):
                    images1, images2 = images1.to(device), images2.to(device)
                    scores = torch.sigmoid(model(images1, images2)).detach().cpu()
                    y[i] = labels[0]
                    y_score[i] = scores[0]
                                        # for test
            #         count+=1
            #         if count >5:
            #             break
            y_hat = (y_score >= 0.5).type(torch.int)
            y = y.numpy()
            y_score = y_score.numpy()
            y_hat = y_hat.numpy()
            p, r, f, _ = precision_recall_fscore_support(y, y_hat)
            try:
              roc_auc = roc_auc_score(y, y_score, average=None)
              print('Epoch {}/{}, P {:.4f}, R {:.4f}, F {:.4f}, AUC {:.4f}'.format(
                  epoch, args.num_epochs, p.mean(), r.mean(), f.mean(), roc_auc.mean()))
              df = np.stack([p, r, f, roc_auc], axis=1)
              df = pd.DataFrame(df, columns=['precision', 'recall', 'f1', 'auc'])
              df.insert(0, 'name', keywords)
              df.to_csv(os.path.join('./output', args.name + '_e{}.csv'.format(epoch)))
            except ValueError:
              pass


    writer.close()
